cwd = join(wd, sets)
if not os.path.exists(join(wd, sets)):
    os.makedirs(join(wd, sets))

# 配置文件生成
def config_data(classes, trainPath, validPath, namesPath):
    dataString = \
'''
classes= %d
train  = %s
valid  = %s
names  = %s
backup = backup
''' \
    %(classes, trainPath, validPath, namesPath)
    return dataString

def config_cfg(classes, batchs):
    cfgString = \
'''
[net]
batch=64
subdivisions=16
width=416
height=416
channels=3
momentum=0.9
decay=0.0005
angle=0
saturation = 1.5
exposure = 1.5
hue=.1

learning_rate=0.001
burn_in=1000
max_batches = {maxBatchs}
policy=steps
steps={maxBatchs_2},{maxBatchs_3}
scales=.1,.1



[convolutional]
batch_normalize=1
filters=32
size=3
stride=1
pad=1
activation=leaky

# Downsample

[convolutional]
batch_normalize=1
filters=64
size=3
stride=2
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=32
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky

[shortcut]
from=-3
activation=linear

# Downsample

[convolutional]
batch_normalize=1
filters=128
size=3
stride=2
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky

[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky

[shortcut]
from=-3
activation=linear

# Downsample

[convolutional]
batch_normalize=1
filters=256
size=3
stride=2
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

[shortcut]
from=-3
activation=linear


[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

[shortcut]
from=-3
activation=linear

# Downsample

[convolutional]
batch_normalize=1
filters=512
size=3
stride=2
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky

[shortcut]
from=-3
activation=linear


[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky

[shortcut]
from=-3
activation=linear


[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky

[shortcut]
from=-3
activation=linear


[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky

[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky

[shortcut]
from=-3
activation=linear


[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky

[shortcut]
from=-3
activation=linear


[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky

[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky

[shortcut]
from=-3
activation=linear

# Downsample

[convolutional]
batch_normalize=1
filters=1024
size=3
stride=2
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=leaky

[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=leaky

[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=leaky

[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=leaky

[shortcut]
from=-3
activation=linear

######################

[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=1024
activation=leaky

[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=1024
activation=leaky

[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=1024
activation=leaky

[convolutional]
size=1
stride=1
pad=1
filters={filters}
activation=linear

[yolo]
mask = 6,7,8
anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
classes={classes}
num=9
jitter=.3
ignore_thresh = .5
truth_thresh = 1
random=1

[route]
layers = -4

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[upsample]
stride=2

[route]
layers = -1, 61



[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=512
activation=leaky

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=512
activation=leaky

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=512
activation=leaky

[convolutional]
size=1
stride=1
pad=1
filters={filters}
activation=linear

[yolo]
mask = 3,4,5
anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
classes={classes}
num=9
jitter=.3
ignore_thresh = .5
truth_thresh = 1
random=1

[route]
layers = -4

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[upsample]
stride=2

[route]
layers = -1, 36



[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky

[convolutional]
size=1
stride=1
pad=1
filters={filters}
activation=linear

[yolo]
mask = 0,1,2
anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
classes={classes}
num=9
jitter=.3
ignore_thresh = .5
truth_thresh = 1
random=1


''' \
    .format(maxBatchs=batchs, maxBatchs_2=int(batchs/2), maxBatchs_3=int(3*batchs/4), filters=(3*(5+classes)),classes=classes)
    return cfgString

# 测试代码
def bbox(train_txt):
    cv.namedWindow('show')
    images = open(train_txt,'r').read().strip().split('\n')
    for path in images[:]:
        img = cv.imread(path)
        if img is None:
            continue
        label = path.replace('/images/', '/labels/').replace('.jpg', '.txt').replace('.jpeg', '.txt').replace('.png', '.txt')
        boxs = np.loadtxt(label)
        if boxs is None:
            continue
        (h, w, _) = img.shape
        boxs = np.reshape(boxs, (boxs.size/5, 5))
        for box in boxs:
            p1 = (int(box[1]*w-box[3]*w/2), int(box[2]*h-box[4]*h/2))
            p2 = (int(box[1]*w+box[3]*w/2), int(box[2]*h+box[4]*h/2))
            img = cv.rectangle(img, p1, p2, (255,0,0), 3)
        cv.imshow('show', img)
        cv.waitKey(2000)


def gif2jpg(file):
    try:
        img = Image.open(file)
        img = img.convert('RGB')
        imgPath = file.replace('.gif', '.jpg')
        img.save(imgPath, 'jpeg')
        os.remove(file)
    except Exception as e:
        print e

def png2jpg(file):
    try:
        img = Image.open(file)
        img = img.convert('RGB')
        imgPath = file.replace('.png', '.jpg')
        img.save(imgPath, 'jpeg')
        os.remove(file)
    except Exception as e:
        print e

def download(url):
    try:
        header = {
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36',
            'Cookie': 'AspxAutoDetectCookieSupport=1'
        }
        req = urllib2.Request(url = url, headers=header)
        binary_data = urllib2.urlopen(req).read()
        imgName = url[url.rfind('/')+1:]
        imgPath = join(wd, sets,'images', imgName)
        
        gif = False
        png = False
        if ord(binary_data[0]) == 0x89 and ord(binary_data[1]) == 0x50: # png
            imgPath = imgPath.replace('.jpg', '.png')
            png = True
        if ord(binary_data[0]) == 0x47 and ord(binary_data[1]) == 0x49: # gif
            imgPath = imgPath.replace('.jpg', '.gif')
            gif = True
        with open(imgPath, 'wb') as temp_file:
            temp_file.write(binary_data)
        
        if gif:
            gif2jpg(imgPath)
        if png:
            png2jpg(imgPath)
        return True
    except Exception as e:
        print "error found",e
        return False

def generate():
    if not os.path.exists(join(wd, sets,'images')):
        os.makedirs(join(wd, sets,'images'))
    if not os.path.exists(join(wd, sets,'labels')):
        os.makedirs(join(wd, sets,'labels'))

    if os.path.exists(join(cwd, 'train.txt')):
        os.remove(join(cwd,'train.txt'))
    train_txt = open(join(cwd,'train.txt'), 'a')
    if os.path.exists(join(cwd, 'valid.txt')):
        os.remove(join(cwd,'valid.txt'))
    valid_txt = open(join(cwd,'valid.txt'), 'a')
    for line in marks[0:]:
        line = line.split(' ')
        if len(line) < 1:
            continue
        if download(line[0]):
            imgName = line[0][line[0].rfind('/')+1:line[0].rfind('.')]
            labelString = ''
            for label in line[1:]:
                label = label.split(',')
                labelString += ' '.join(label[0:-1])
                labelString += '\n'
            open(join(wd, sets,'labels/%s.txt'%(imgName)), 'w').write(labelString)
            if random() > 0.9:
                valid_txt.write(join(wd, sets,'images/%s'%(line[0][line[0].rfind('/')+1:])) + '\n')
            else:
                train_txt.write(join(wd, sets,'images/%s'%(line[0][line[0].rfind('/')+1:])) + '\n')

    train_txt.close()
    valid_txt.close()

def check():
    images = listdir(join(wd, sets,'images'))
    labels = listdir(join(wd, sets,'labels'))
    train_txt = open(join(cwd, 'train.txt'),'r').read().strip().split('\n')
    valid_txt = open(join(cwd, 'valid.txt'),'r').read().strip().split('\n')

    if len(images) != len(labels):
        print 'count not match: images is %d, label is %d'%(len(images),len(labels))
        return False
    else:
        print 'here is %d samples'%(len(images))
    
    for image in images[:]:
        name = image[:image.rfind('.')]
        if (name + '.txt') not in labels:
            print '%s.jpg is not match %s.txt'%(name,name)
    for train in train_txt:
        if not os.path.exists(train):
            print '%s is not found'%(train)
    for valid in valid_txt:
        if not os.path.exists(valid):
            print '%s is not found'%(valid)

def geConfig(classes, trainPath, validPath, namesPath, batchs):
    dataString = config_data(classes, trainPath, validPath, namesPath)
    open(join(wd, sets, 'voc.data'), 'w').write(dataString)
    cfgString = config_cfg(classes, batchs)
    open(join(wd, sets, 'voc.cfg'), 'w').write(cfgString)

def savenames():
    open(join(cwd, 'names.txt'), 'w').write("\n".join(names))

# 调用代码
generate()
savenames()
check()
geConfig(1, join(cwd, 'train.txt'), join(cwd, 'valid.txt'), join(cwd, 'names.txt'), 4000)
# bbox(join(cwd, 'train.txt'))

print './darknet detector train %s %s darknet53.conv.74'%(join(cwd, 'voc.data'), join(cwd, 'voc.cfg'))